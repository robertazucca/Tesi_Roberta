{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from datetime import datetime, date\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [\"03-05-21\", \"10-05-21\", \"17-05-21\", \"24-05-21\", \"31-05-21\", \"07-06-21\", \"14-06-21\", \"21-06-21\", \"28-06-21\", \"05-07-21\", \"12-07-21\", \"19-07-21\", \"26-07-21\", \"02-08-21\", \"09-08-21\",\n",
    "\"16-08-21\", \"23-08-21\", \"30-08-21\", \"06-09-21\"]\n",
    "chan_dir = 'channels/'\n",
    "nod_dir = 'nodes/'\n",
    "files_dir = ['datasetLun/']\n",
    "historic_filenames = [str(date) + '.json' for date in dates]\n",
    "channels_dir = ['channels/ln_' + str(date) + '.json__edges.json' for date in dates]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"str\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ROBERT~1\\AppData\\Local\\Temp/ipykernel_3052/1893692595.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#ogni dataset lo divido in file dei canali e file dei nodi per passarli per la costruzione dei grafi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhistoric_filenames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mdata_channels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdf_channels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_channels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrecord_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"edges\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate list (not \"str\") to list"
     ]
    }
   ],
   "source": [
    "#ogni dataset lo divido in file dei canali e file dei nodi per passarli per la costruzione dei grafi\n",
    "for filename in historic_filenames:\n",
    "    with open(files_dir + filename,'r', encoding='utf-8') as data_file:    \n",
    "        data_channels = json.load(data_file)\n",
    "    df_channels = pd.json_normalize(data_channels,record_path = [\"edges\"])\n",
    "    #display(df_channels)\n",
    "\n",
    "    #to_write_n = df_channels.to_json(orient='records', indent = 1)\n",
    "    #with open (filename + '_edges.json', \"w\") as file:\n",
    "    #    file.write(to_write_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in historic_filenames:\n",
    "    with open (filename,'r', encoding='utf-8') as data_file:\n",
    "        data_nodes = json.load(data_file)\n",
    "    df_nodes = pd.json_normalize(data_nodes, record_path=[\"nodes\"])\n",
    "#tengo solo le prime 5 colonne\n",
    "    df_drop_n = df_nodes.drop(df_nodes.iloc[:,5:68],axis=1)\n",
    "\n",
    "    to_write_n = df_drop_n.to_json(orient='records', indent = 1)\n",
    "    with open (filename + '__nodes.json', \"w\") as file:\n",
    "        file.write(to_write_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divido i dataset relativi ai mesi di aprile e meggio -> quelli che hanno pi√π dati\n",
    "\n",
    "dates_new = [\"23-04-21\", \"24-04-21\", \"25-04-21\", \"26-04-21\", \"27-04-21\", \"28-04-21\", \"29-04-21\", \"30-04-21\", \"01-05-21\", \"02-05-21\", \"03-05-21\", \"04-05-21\", \"05-05-21\", \"06-05-21\", \"07-05-21\",\n",
    "\"08-05-21\", \"09-05-21\", \"10-05-21\"]\n",
    "\n",
    "dir_dataset = 'datasetAprileMaggio/'\n",
    "files_dataset = ['ln_' + str(date) + '.json' for date in dates_new]\n",
    "dir_channels = 'datasetAprileMaggio/channels/'\n",
    "dir_nodes = 'datasetAprileMaggio/nodes/'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_drop_n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ROBERT~1\\AppData\\Local\\Temp/ipykernel_3052/267599136.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mto_write_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_channels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'records'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mto_write_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_drop_n\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'records'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdir_channels\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'channels_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_drop_n' is not defined"
     ]
    }
   ],
   "source": [
    "#ogni dataset lo divido in file dei canali e file dei nodi per passarli per la costruzione dei grafi\n",
    "for filename in files_dataset:\n",
    "    with open(dir_dataset + filename,'r', encoding='utf-8') as data_file:    \n",
    "        data = json.load(data_file)\n",
    "    df_channels = pd.json_normalize(data,record_path = [\"edges\"])\n",
    "    df_nodes = pd.json_normalize(data,record_path = [\"nodes\"])\n",
    "    df_drop_n = df_nodes.drop(df_nodes.iloc[:,5:68],axis=1)\n",
    "\n",
    "\n",
    "    #display(df_channels)\n",
    "\n",
    "    to_write_c = df_channels.to_json(orient='records', indent = 1)\n",
    "    to_write_n = df_drop_n.to_json(orient='records', indent = 1)\n",
    "\n",
    "    with open (dir_channels + 'channels_' + filename, \"w\") as file:\n",
    "        file.write(to_write_c)\n",
    "    with open (dir_nodes + 'nodes_' + filename, \"w\") as file:\n",
    "        file.write(to_write_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in dir_dataset:\n",
    "    with open (filename,'r', encoding='utf-8') as data_file:\n",
    "        data_nodes = json.load(data_file)\n",
    "    df_nodes = pd.json_normalize(data_nodes, record_path=[\"nodes\"])\n",
    "#tengo solo le prime 5 colonne\n",
    "    df_drop_n = df_nodes.drop(df_nodes.iloc[:,5:68],axis=1)\n",
    "\n",
    "    to_write_n = df_drop_n.to_json(orient='records', indent = 1)\n",
    "    with open (filename + '__nodes.json', \"w\") as file:\n",
    "        file.write(to_write_n)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "90e315135b2281b752a7f250632c791a0dae1003299eabe9b8abad9d58b3e8e7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
