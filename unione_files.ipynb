{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import operator\n",
    "import scipy\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from  operator import itemgetter\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "import collections\n",
    "from collections import Counter\n",
    "from statistics import mean #per la media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_new = [\"23-04-21\", \"24-04-21\", \"25-04-21\", \"26-04-21\", \"27-04-21\", \"28-04-21\", \"29-04-21\", \"30-04-21\", \"01-05-21\", \"02-05-21\", \"03-05-21\", \"04-05-21\", \"05-05-21\", \"06-05-21\", \"07-05-21\",\n",
    "\"08-05-21\", \"09-05-21\", \"10-05-21\"]\n",
    "week_1_dates = [\"23-04-21\", \"24-04-21\", \"25-04-21\"]\n",
    "week_2_dates = [\"26-04-21\", \"27-04-21\", \"28-04-21\", \"29-04-21\", \"30-04-21\", \"01-05-21\", \"02-05-21\"]\n",
    "week_3_dates = [\"03-05-21\", \"04-05-21\", \"05-05-21\", \"06-05-21\", \"07-05-21\", \"08-05-21\", \"09-05-21\"]\n",
    "week_4_dates = [\"10-05-21\"]\n",
    "\n",
    "dir_dataset = 'datasetAprileMaggio/'\n",
    "files_dataset = ['ln_' + str(date) + '.json' for date in week_1_dates]\n",
    "dir_channels = 'datasetAprileMaggio/channels/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ROBERT~1\\AppData\\Local\\Temp/ipykernel_9928/4019670384.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mdf_union\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"node1_policy.fee_base_msat_history\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_channels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"node1_policy.fee_base_msat\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_union\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mdf_already_taken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_channels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf_union\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mfrom_dict\u001b[1;34m(cls, data, orient, dtype, columns)\u001b[0m\n\u001b[0;32m   1591\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"only recognize index or columns for orient\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1593\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1595\u001b[0m     def to_numpy(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m             \u001b[1;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[1;31m# TODO: can we get rid of the dt64tz special case above?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m     return arrays_to_mgr(\n\u001b[0m\u001b[0;32m    465\u001b[0m         \u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"All arrays must be of the same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "df_union = {\n",
    "  \"node1_policy.min_htlc_history\":[],\n",
    "  \"node1_policy.fee_base_msat_history\":[],\n",
    "  \"node1_policy.fee_rate_milli_msat_history\":[],\n",
    "  \"node2_policy.min_htlc_history\":[],\n",
    "  \"node2_policy.fee_base_msat_history:\":[],\n",
    "  \"node2_policy.fee_rate_milli_msat_history\":[],\n",
    " }\n",
    "\n",
    "\n",
    "i = 0\n",
    "for filename in files_dataset:\n",
    "\n",
    "    with open (dir_channels+'channels_' + filename) as data_file:    \n",
    "        data_channels = json.load(data_file)\n",
    "    df_channels = pd.json_normalize(data_channels)\n",
    "    #df_channels['last_seen'] = dates[i]\n",
    "    i+=1\n",
    "    \n",
    "    \n",
    "    #devo prendere i canali presenti nel grande file e aggiornarli, oppure aggiungerne degli altri\n",
    "    #if not os.path.exists(blend_filename):\n",
    "    #if i == 1:\n",
    "        #il file non esiste, quindi sto considerando il primo giorno e devo solo inserire nel file grande tutti i canali aperti\n",
    "    df_union[\"node1_policy.min_htlc_history\"].append((df_channels[\"node1_policy.min_htlc\"],filename[4:8]))\n",
    "    df_union[\"node1_policy.fee_base_msat_history\"].append((df_channels[\"node1_policy.fee_base_msat\"],filename[4:8]))\n",
    "\n",
    "    pd.DataFrame.from_dict(df_union)\n",
    "\n",
    "    df_already_taken = pd.concat(df_channels,df_union)\n",
    "\n",
    "\n",
    "    display(df_already_taken)\n",
    "        #df_open_channels = df_channels[df_channels[\"close.time\"].isnull()]\n",
    "        #to_write = df_open_channels.to_json(orient='records', indent = 1)\n",
    "        #with open(blend_filename, \"w\") as f:\n",
    "        #   f.write(to_write)\n",
    "    #else:\n",
    "        #il file esiste, devo aggiornare i canali che ho\n",
    "        #with open(blend_filename) as f:\n",
    "        #    already_taken = json.load(f)\n",
    "        #df_already_taken = pd.json_normalize(already_taken)\n",
    "        #print(\"Numero di canali nel grande file:\", len(df_already_taken))\n",
    "\n",
    "        #prendo i canali del nuovo dataset che sono anche nel grande file\n",
    "        #in_common = df_channels[df_channels['channel_id'].isin(df_already_taken['channel_id'])]\n",
    "        #print(\"In comune tra il giorno corrente e il grande file:\" ,len(in_common))\n",
    "        #filtro le righe in modo tale da eliminare i record che devo riaggiungere aggiornati al giorno dopo\n",
    "        #df_already_taken_dropped = df_already_taken[~df_already_taken['short_channel_id'].isin(in_common['short_channel_id'])]\n",
    "        #print(\"Numero di canali del grande file senza quelli in comune:\", len(df_already_taken_dropped))\n",
    "        #new_df_already_taken = pd.concat([df_already_taken_dropped, in_common])\n",
    "        #print(\"Numero di canali del grande file dopo aver aggiunto i canali aggiornati:\", len(new_df_already_taken))\n",
    "\n",
    "        #a questo punto devo prendere i canali aperti del file del giorno, che non si trovano nel grande file, ed aggiungerli\n",
    "        #df_open_channels = df_channels[df_channels[\"close.time\"].isnull()]\n",
    "        #print(\"Numero di canali aperti questo giorno:\", len(df_open_channels))\n",
    "        #df_open_today = df_open_channels[~df_open_channels['short_channel_id'].isin(new_df_already_taken['short_channel_id'])]\n",
    "        #print(\"Il numero di canali risultanti aperti in questo giorno e non ho ancora aggiunti Ã¨:\", len(df_open_today), \"su totali:\",len(df_open_channels))\n",
    "\n",
    "        #new_df_already_taken = pd.concat([new_df_already_taken, df_open_today])\n",
    "        #print(\"Numero di canali totali dopo questa iterazione\", len(new_df_already_taken))\n",
    "\n",
    "        #df_already_taken = new_df_already_taken\n",
    "        #metto insieme i canali che devo memorizzare\n",
    "        #to_write = new_df_already_taken.to_json(orient='records', indent = 1)\n",
    "        #with open(blend_filename, \"w\") as f:\n",
    "        #   f.write(to_write)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "042a03a709baefc4696b89b1773a1551f77cb3b721a024643b48fc758e4895f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
